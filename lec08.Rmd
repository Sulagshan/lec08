---
title: "STA286 Lecture 08"
author: "Neil Montgomery"
date: "Last edited: `r format(Sys.time(), '%Y-%m-%d %H:%M')`"
output: 
  beamer_presentation:
    keep_tex: TRUE
    incremental: TRUE
    df_print: tibble
    fig_caption: FALSE
classoption: aspectratio=169
header-includes:
- \renewcommand{\le}{\leqslant}
- \renewcommand{\ge}{\geqslant}
- \renewcommand\P[1]{P{\left(#1\right)}}
- \newcommand\F[1]{F_{\tiny{#1}}}
- \newcommand\f[1]{f_{\tiny{#1}}}
- \newcommand\p[1]{p_{\tiny{#1}}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
options(tibble.width=70)
```

## a note on "identically distributed"

The distribution is all we care about.

So if $X_1$ and $X_2$ have the same distributions, they are effectively the same (even if they are not the same functions.)

\pause For example, roll a fair die so that $S = \{1,2,3,4,5,6\}.$ 

\pause Define:
$$X_1 = \begin{cases}
1 &: 3 \text{ or } 4 \text{ appears }\\
0 &: \text{otherwise}
\end{cases} \qquad \text{and} \qquad X_2 = \begin{cases}
1 &: 5 \text{ or } 6 \text{ appears }\\
0 &: \text{otherwise}
\end{cases}$$

\pause $X_1$ and $X_2$ are not the same functions. But the have the same p.m.f.:
$$p(x) = \left(\frac{1}{3}\right)^x\left(\frac{2}{3}\right)^{1-x},\ x\in\{0,1\}.$$
We say $X_1$ and $X_2$ are *identically distributed*.

# joint distributions

## more than one random variable at a time

A dataset will usually have more than one variable. They might be modeled at the same time.

\pause Certainly a dataset will have more than one observation, so considering multiple random variables is essential.

\pause The approach is to consider a random vector such as $(X,Y)$, $(X_1,X_2)$, $(X_1,X_2,\ldots X_n)$ and so on. At first we'll consider two at a time: $(X,Y)$.

\pause As usual we are interested in the *distribution* of, say, $(X,Y)$, which now means the collection of things like $P((X,Y) \in A)$ for $A \in \mathbb{R}^2$.

\pause Main interest is in statements like:
$$P(X = x, Y = y) \qquad \text{ or } \qquad P(a < X < b, c < Y < d)$$
where the "comma" notation is a compact way of writing, say:
$$P(\{X = x \} \cap \{ Y =y \})$$

## cdf, pmf, pdf

\pause The natural ways to characterize joint distributions are still probability mass functions and probability density functions.

\pause They also have cdfs:
$$F(x,y) = P(X \le x, Y \le y)$$
but we won't focus much on these.

## a joint pmf

A gas distribution company has pipes with diameters 1, 1.5, and 1.75 inches. The pipes are used at pressures of 2, 1, and 0.5 pounds per square inch.

Pick a pipe at random and denote its diameter by $X$ and its pressure by $Y$.

\pause $X$ and $Y$ might have the following *joint probability mass function*:

```{r, results='asis'}
library(tidyverse)
library(xtable)
diams <- c("1", "1.5", "1.75")
psi <- c("0.5", "1", "2")
Counts <- c(75, 100, 150, 110, 80, 90, 160, 140, 95)
gas <- data.frame(expand.grid(Diameter=diams, Pressure=psi), Counts=Counts,
                  check.names = FALSE)
gas_joint <- prop.table(xtabs(Counts ~ Pressure + Diameter, gas))

addtorow <- list()
addtorow$pos <- list(0, 0)
addtorow$command <- c("& \\multicolumn{3}{c}{$X$} \\\\\n",
"$Y$ & 1 & 1.5 & 1.75 \\\\\n")

print(xtable(gas_joint, digits=3), add.to.row=addtorow, comment=FALSE,
      include.colnames = FALSE)
```

\pause e.g. the probability that a randomly selected pipe has diamter $X=1.5$ and pressure $Y=0.5$ is:
$$P(X = 1.5, Y = 0.5) = 0.1$$

## joint pmf properties; joint density

A joint pmf is still just a pmf. It must be non-negative, and its positive values must add up to 1. 

\pause $(X,Y)$ are (jointly) continuous if they have a joint density $f$ such that:
$$P(a < X < b, c < Y < d) = \int\limits_a^b\int\limits_c^d f(x,y)\,dxdy$$

\pause A joint density also must be non-negative and integrate to 1 over all $\mathbb{R}^2$.

## joint density example - I 

Two electronic components fail at times $X$ and $Y$ according to the following joint density (measured in years):

$$f(x,y) \begin{cases}2e^{-x-2y} &: x > 0, y > 0\\
0 &: \text{otherwise}\end{cases}$$

\pause Is this actually a valid joint density? It is non-negative, and:
$$\iint\limits_\mathbb{R^2} f(x,y) = \int\limits_0^\infty\int\limits_0^\infty 2e^{-x-2y}\,dxdy = \int\limits_0^\infty e^{-x}\,dx \int\limits_0^\infty 2e^{-2y}\,dy = 1$$

## joint density example - II

What is the probability that the first component fails before the second? In other words, what is $P(X < Y)$?

\pause Answer: integrate the joint density over the region where $x < y$. 

\begin{align*}
\int\limits_0^\infty\int\limits_0^y 2e^{-x-2y}\,dx\,dy &= 
  \onslide<2->{\int\limits_0^\infty \left[-2e^{-x-2y}\right]_{x=0}^y\,dy\\}
  \onslide<3->{&= \int\limits_0^\infty 2e^{-2y} - 2e^{-3y}\,dy\\}
  \onslide<4->{&= \left[-e^{-2y} + \frac{2}{3} e^{-3y}\right]_{y=0}^\infty\\}
  \onslide<5->{&= \frac{1}{3}}
\end{align*}

## marginal distributions 

A joint distribution contains *all* information about both $X$ and $Y$ together.

"*All*" includes information about $X$ alone, and about $Y$ alone. And it's easy to get this information from the joint distribution.

\pause For example, look again at the joint distribution for diameter $X$ and pressure $Y$ of the randomly selected pipe:

```{r, results='asis'}
print(xtable(gas_joint, digits=3), add.to.row=addtorow, comment=FALSE,
      include.colnames = FALSE)
```

\pause A statement about $X$ alone could be something like $P(X = 1)$, which is just `r paste0(gas_joint[,1], collapse=" + ")` = `r sum(gas_joint[,1])`

## marginal pmf

Given the joint pmf $p(x,y)$ for $X$ and $Y$, the marginal pmf for $X$ is:
$$\p{X}(x) = \sum\limits_y p(x,y)$$

\pause Here are the "marginals" for both $X$ and $Y$ in the gas example:

```{r, results='asis'}
addtorow_m <- list()
addtorow_m$pos <- list(0, 0)
addtorow_m$command <- c("& \\multicolumn{3}{c|}{$X$} & \\\\\n",
"$Y$ & 1 & 1.5 & 1.75 & Marginal\\\\\n")

print(xtable(addmargins(gas_joint, FUN=list(Marginal=sum), quiet=TRUE), digits=3,
             align="r|rrr|r"), add.to.row=addtorow_m, comment=FALSE,
      include.colnames = FALSE, hline.after = c(0,3))
```

## marginal density

Reconsider the two electronic components example:
$$f(x,y) \begin{cases}2e^{-x-2y} &: x > 0, y > 0\\
0 &: \text{otherwise}\end{cases}$$

A statement about, say, $Y$ alone might be $P(Y > 1)$, which would be the integral on the entire half plane $y > 1$:
$$\iint\limits_{y > 1} f(x,y)\,dxdy
\onslide<2->{=\int\limits_1^\infty\int\limits_0^\infty 2e^{-x-2y}\,dxdy}
\onslide<3->{=\int\limits_1^\infty 2e^{-2y}\,dy}
\onslide<4->{=e^{-2}}$$

## marginal density

Given the joint density $f(x,y)$ for $X$ and $Y$, the marginal density for $X$ is:
$$\f{X}(x) = \int\limits_{-\infty}^\infty f(x,y)\,dy$$

\pause Marginal densities can't really be visualized. At best they are a "projection" onto one or the other axis.

\pause e.g. Consider the following function:
$$f(x,y) = \begin{cases}
2 &: 0 < x < 1,\ 0 < y < 1,\ x + y < 1\\
0 &: \text{otherwise}
\end{cases}$$
\pause This is a valid density. Let $X$ and $Y$ have this joint density.

## marginal density

The marginal density for $X$ will be 0 outside $x \in (0,1)$. Otherwise:
\begin{align*}\f{X}(x) &= \int\limits_{-\infty}^\infty f(x,y)\,dy\\
\onslide<2->{&= \int\limits_0^x 2\,dy\\}
\onslide<3->{&= 2x}
\end{align*}

## conditional distributions

Consider again the gas pipe joint (and marginal) distributions:
```{r, results='asis'}
print(xtable(addmargins(gas_joint, FUN=list(Marginal=sum), quiet=TRUE), digits=3,
             align="r|rrr|r"), add.to.row=addtorow_m, comment=FALSE,
      include.colnames = FALSE, hline.after = c(0,3))
```

\pause What are the probabilities of $X$ taking on any of its three possible values *given $Y=2$, say.*

\pause $$P(X=1 | Y=2) = frac{P(X=1, Y=2)}{P(Y=2)} = \frac{`r gas_joint[3,1]`}{`r sum(gas_joint[3,])`}$$

\pause $$P(X=1.5 | Y=2) = \frac{`r gas_joint[3,2]`}{`r sum(gas_joint[3,])`} \qquad \qquad P(X=1.75 | Y=2) = \frac{`r gas_joint[3,3]`}{`r sum(gas_joint[3,])`} $$

## conditional pmf and conditional density

Given the joint pmf $p(x,y)$ for $X$ and $Y$, the conditional pmf for $X$ given $Y=y$ is:
$$p(x|y) = \frac{p(x,y)}{\p{Y}(y)},$$
provided $P(Y=y) > 0.$

\pause Given the joint density $f(x,y)$ for $X$ and $Y$, the conditional density for $X$ given $Y=y$ is:
$$f(x|y) = \frac{f(x,y)}{\f{Y}(y)},$$
provided $\f{Y}(y) > 0.$

## conditional density example

Reconsider the two electronic components example:
$$f(x,y) \begin{cases}2e^{-x-2y} &: x > 0, y > 0\\
0 &: \text{otherwise}\end{cases}$$
The marginal density for $Y$ is $2e^{-2y}$ on $y > 0$. 

\pause The conditional density for $X$ given $Y = 2$ is:

\pause $$e^{-x} \text{ on } x > 0$$

\pause Heck, the conditional density for $X$ given $Y$ equals anything greater than 0 is still always $e^{-x}$ on $x > 0.$ 

\pause That's because $X$ and $Y$ have a special relationship\ldots to be revisited.

## conditional density example

Consider again the following function:
$$f(x,y) = \begin{cases}
2 &: 0 < x < 1,\ 0 < y < 1,\ x + y < 1\\
0 &: \text{otherwise}
\end{cases}$$
The marginal density for $X$ was found to be $\f{X}(x) = 2x$ for $0 < x < 1$.

\pause The condtional density of $Y$ given $X=0.5$ will be 0 when $y$ is outside $(0, 0.5)$. Otherwise it will be:
$$\f{Y}(y) = \frac{2}{2\cdot 0.5} = 2$$

\pause Unlike marginal densities, conditional densities can sometimes be visualized. They are just "slices" of the joint density.

